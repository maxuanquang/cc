{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "close-reconstruction",
   "metadata": {},
   "source": [
    "# Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tired-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "departmental-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_l1(x, q=0.5, eps=1e-2):\n",
    "    x = torch.pow((x.pow(2) + eps), q)\n",
    "    x = x.mean()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "incorrect-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_l1_per_pix(x, q=0.5, eps=1e-2):\n",
    "    x = torch.pow((x.pow(2) + eps), q)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "valued-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Jonas Wulff\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous(), requires_grad=False)\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    return ssim_map\n",
    "    #if size_average:\n",
    "    #    return ssim_map.mean()\n",
    "    #else:\n",
    "    #    return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "\n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 13, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "spare-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_img = torch.rand(4,3,256,832)\n",
    "\n",
    "ref_imgs = [torch.rand(4,3,256,832),torch.rand(4,3,256,832)]\n",
    "\n",
    "occ_masks = torch.rand(4,4,640,640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "recreational-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = 0\n",
    "h, w = 640, 640\n",
    "\n",
    "tgt_img_scaled = nn.functional.adaptive_avg_pool2d(tgt_img, (h, w))\n",
    "ref_imgs_scaled = [nn.functional.adaptive_avg_pool2d(ref_img, (h, w)) for ref_img in ref_imgs]\n",
    "\n",
    "weight = 0.5\n",
    "wssim = 0.85\n",
    "\n",
    "ssim_losses = []\n",
    "photometric_losses = []\n",
    "\n",
    "for i, ref_img in enumerate(ref_imgs_scaled):\n",
    "\n",
    "    valid_pixels = 1 - (ref_img == 0).prod(1, keepdim=True).type_as(ref_img)\n",
    "    diff = (tgt_img_scaled - ref_img) * valid_pixels\n",
    "    ssim_loss = 1 - ssim(tgt_img_scaled, ref_img) * valid_pixels\n",
    "    oob_normalization_const = valid_pixels.nelement()/valid_pixels.sum()\n",
    "\n",
    "    assert((oob_normalization_const == oob_normalization_const).item() == 1)\n",
    "\n",
    "    diff = diff *(1-occ_masks[:,i:i+1]).expand_as(diff)\n",
    "    ssim_loss = ssim_loss*(1-occ_masks[:,i:i+1]).expand_as(ssim_loss)\n",
    "\n",
    "    # reconstruction_loss +=  oob_normalization_const*((1- wssim)*robust_l1_per_pix(diff, q=qch) + weight*wssim*ssim_loss).min() + lambda_oob*robust_l1(1 - valid_pixels, q=qch)\n",
    "    ssim_losses.append(oob_normalization_const*weight*wssim*ssim_loss)\n",
    "    photometric_losses.append(oob_normalization_const*(1 - wssim)*robust_l1(diff))\n",
    "    # assert((reconstruction_loss == reconstruction_loss).item() == 1)\n",
    "    #weight /= 2.83\n",
    "\n",
    "ssim_losses = torch.stack(ssim_losses)\n",
    "photometric_losses = torch.stack(photometric_losses)\n",
    "reconstruction_loss = torch.min(ssim_losses,0)[0].mean() + torch.mean(photometric_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "portable-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = 0\n",
    "h, w = 640, 640\n",
    "\n",
    "tgt_img_scaled = nn.functional.adaptive_avg_pool2d(tgt_img, (h, w))\n",
    "ref_imgs_scaled = [nn.functional.adaptive_avg_pool2d(ref_img, (h, w)) for ref_img in ref_imgs]\n",
    "\n",
    "weight = 0.5\n",
    "wssim = 0.85\n",
    "\n",
    "pe_losses = []\n",
    "\n",
    "for i, ref_img in enumerate(ref_imgs_scaled):\n",
    "\n",
    "    valid_pixels = 1 - (ref_img == 0).prod(1, keepdim=True).type_as(ref_img)\n",
    "    diff = (tgt_img_scaled - ref_img) * valid_pixels\n",
    "    ssim_loss = 1 - ssim(tgt_img_scaled, ref_img) * valid_pixels\n",
    "    oob_normalization_const = valid_pixels.nelement()/valid_pixels.sum()\n",
    "\n",
    "    assert((oob_normalization_const == oob_normalization_const).item() == 1)\n",
    "\n",
    "    diff = diff *(1-occ_masks[:,i:i+1]).expand_as(diff)\n",
    "    ssim_loss = ssim_loss*(1-occ_masks[:,i:i+1]).expand_as(ssim_loss)\n",
    "\n",
    "    # reconstruction_loss +=  oob_normalization_const*((1- wssim)*robust_l1_per_pix(diff, q=qch) + weight*wssim*ssim_loss).min() + lambda_oob*robust_l1(1 - valid_pixels, q=qch)\n",
    "    pe = oob_normalization_const*weight*wssim*ssim_loss + oob_normalization_const*(1 - wssim)*robust_l1_per_pix(diff, q=qch) + lambda_oob*robust_l1_per_pix(1 - valid_pixels, q=qch) \n",
    "    pe = pe.mean(1)\n",
    "    pe_losses.append(pe)\n",
    "    # assert((reconstruction_loss == reconstruction_loss).item() == 1)\n",
    "    #weight /= 2.83\n",
    "\n",
    "pe_losses = torch.stack(pe_losses)\n",
    "\n",
    "pe_losses = pe_losses.min(0)[0]\n",
    "\n",
    "reconstruction_loss = pe_losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "brave-douglas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 640, 640])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssim_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "exceptional-custom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0193)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photometric_losses.min(0)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "rational-beach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 640, 640])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_l1_per_pix(diff).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "nonprofit-boundary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 640, 640])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssim(tgt_img_scaled, ref_img).mean(1, True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ordinary-differential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1600)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_l1(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss +=  \n",
    "(1- wssim)*robust_l1(diff, q=qch) + wssim*ssim_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dependent-canberra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 832])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quang = [tgt_img.mean(1,True),tgt_img.mean(1,True),tgt_img.mean(1,True)]\n",
    "torch.cat(quang,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "governing-collaboration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1539)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "still-caribbean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1528)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "standard-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometric_reconstruction_loss(tgt_img, ref_imgs, intrinsics, intrinsics_inv, depth, explainability_mask, pose, rotation_mode='euler', padding_mode='zeros', lambda_oob=0, qch=0.5, wssim=0.5):\n",
    "    if type(explainability_mask) not in [tuple, list]:\n",
    "        explainability_mask = [explainability_mask]\n",
    "    if type(depth) not in [list, tuple]:\n",
    "        depth = [depth]\n",
    "\n",
    "    loss = 0\n",
    "    for d, mask in zip(depth, explainability_mask):\n",
    "        occ_masks = depth_occlusion_masks(d, pose, intrinsics, intrinsics_inv)\n",
    "        loss += one_scale(d, mask, occ_masks)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-saver",
   "metadata": {},
   "source": [
    "# Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "automatic-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def downsample_conv(in_planes, out_planes, kernel_size=3):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=2, padding=(kernel_size-1)//2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_planes, out_planes, kernel_size=kernel_size, padding=(kernel_size-1)//2),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def predict_disp(in_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, 1, kernel_size=3, padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv(in_planes, out_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def upconv(in_planes, out_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def crop_like(input, ref):\n",
    "    assert(input.size(2) >= ref.size(2) and input.size(3) >= ref.size(3))\n",
    "    return input[:, :, :ref.size(2), :ref.size(3)]\n",
    "\n",
    "\n",
    "class DispNetS6(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=10, beta=0.01):\n",
    "        super(DispNetS6, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        conv_planes = [32, 64, 128, 256, 512, 512, 512]\n",
    "        self.conv1 = downsample_conv(3,              conv_planes[0], kernel_size=7)\n",
    "        self.conv2 = downsample_conv(conv_planes[0], conv_planes[1], kernel_size=5)\n",
    "        self.conv3 = downsample_conv(conv_planes[1], conv_planes[2])\n",
    "        self.conv4 = downsample_conv(conv_planes[2], conv_planes[3])\n",
    "        self.conv5 = downsample_conv(conv_planes[3], conv_planes[4])\n",
    "        self.conv6 = downsample_conv(conv_planes[4], conv_planes[5])\n",
    "        self.conv7 = downsample_conv(conv_planes[5], conv_planes[6])\n",
    "\n",
    "        upconv_planes = [512, 512, 256, 128, 64, 32, 16]\n",
    "        self.upconv7 = upconv(conv_planes[6],   upconv_planes[0])\n",
    "        self.upconv6 = upconv(upconv_planes[0], upconv_planes[1])\n",
    "        self.upconv5 = upconv(upconv_planes[1], upconv_planes[2])\n",
    "        self.upconv4 = upconv(upconv_planes[2], upconv_planes[3])\n",
    "        self.upconv3 = upconv(upconv_planes[3], upconv_planes[4])\n",
    "        self.upconv2 = upconv(upconv_planes[4], upconv_planes[5])\n",
    "        self.upconv1 = upconv(upconv_planes[5], upconv_planes[6])\n",
    "\n",
    "        self.iconv7 = conv(upconv_planes[0] + conv_planes[5], upconv_planes[0])\n",
    "        self.iconv6 = conv(upconv_planes[1] + conv_planes[4], upconv_planes[1])\n",
    "        self.iconv5 = conv(upconv_planes[2] + conv_planes[3], upconv_planes[2])\n",
    "        self.iconv4 = conv(upconv_planes[3] + conv_planes[2], upconv_planes[3])\n",
    "        self.iconv3 = conv(1 + upconv_planes[4] + conv_planes[1], upconv_planes[4])\n",
    "        self.iconv2 = conv(1 + upconv_planes[5] + conv_planes[0], upconv_planes[5])\n",
    "        self.iconv1 = conv(1 + upconv_planes[6], upconv_planes[6])\n",
    "\n",
    "        self.predict_disp6 = predict_disp(upconv_planes[1])\n",
    "        self.predict_disp5 = predict_disp(upconv_planes[2])\n",
    "        self.predict_disp4 = predict_disp(upconv_planes[3])\n",
    "        self.predict_disp3 = predict_disp(upconv_planes[4])\n",
    "        self.predict_disp2 = predict_disp(upconv_planes[5])\n",
    "        self.predict_disp1 = predict_disp(upconv_planes[6])\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv1 = self.conv1(x)\n",
    "        out_conv2 = self.conv2(out_conv1)\n",
    "        out_conv3 = self.conv3(out_conv2)\n",
    "        out_conv4 = self.conv4(out_conv3)\n",
    "        out_conv5 = self.conv5(out_conv4)\n",
    "        out_conv6 = self.conv6(out_conv5)\n",
    "        out_conv7 = self.conv7(out_conv6)\n",
    "\n",
    "        out_upconv7 = crop_like(self.upconv7(out_conv7), out_conv6)\n",
    "        concat7 = torch.cat((out_upconv7, out_conv6), 1)\n",
    "        out_iconv7 = self.iconv7(concat7)\n",
    "\n",
    "        out_upconv6 = crop_like(self.upconv6(out_iconv7), out_conv5)\n",
    "        concat6 = torch.cat((out_upconv6, out_conv5), 1)\n",
    "        out_iconv6 = self.iconv6(concat6)\n",
    "        disp6 = self.alpha * self.predict_disp6(out_iconv6) + self.beta\n",
    "\n",
    "        out_upconv5 = crop_like(self.upconv5(out_iconv6), out_conv4)\n",
    "        concat5 = torch.cat((out_upconv5, out_conv4), 1)\n",
    "        out_iconv5 = self.iconv5(concat5)\n",
    "        disp5 = self.alpha * self.predict_disp5(out_iconv5) + self.beta\n",
    "\n",
    "        out_upconv4 = crop_like(self.upconv4(out_iconv5), out_conv3)\n",
    "        concat4 = torch.cat((out_upconv4, out_conv3), 1)\n",
    "        out_iconv4 = self.iconv4(concat4)\n",
    "        disp4 = self.alpha * self.predict_disp4(out_iconv4) + self.beta\n",
    "\n",
    "        out_upconv3 = crop_like(self.upconv3(out_iconv4), out_conv2)\n",
    "        disp4_up = crop_like(nn.functional.upsample(disp4, scale_factor=2, mode='bilinear'), out_conv2)\n",
    "        concat3 = torch.cat((out_upconv3, out_conv2, disp4_up), 1)\n",
    "        out_iconv3 = self.iconv3(concat3)\n",
    "        disp3 = self.alpha * self.predict_disp3(out_iconv3) + self.beta\n",
    "\n",
    "        out_upconv2 = crop_like(self.upconv2(out_iconv3), out_conv1)\n",
    "        disp3_up = crop_like(nn.functional.upsample(disp3, scale_factor=2, mode='bilinear'), out_conv1)\n",
    "        concat2 = torch.cat((out_upconv2, out_conv1, disp3_up), 1)\n",
    "        out_iconv2 = self.iconv2(concat2)\n",
    "        disp2 = self.alpha * self.predict_disp2(out_iconv2) + self.beta\n",
    "\n",
    "        out_upconv1 = crop_like(self.upconv1(out_iconv2), x)\n",
    "        disp2_up = crop_like(nn.functional.upsample(disp2, scale_factor=2, mode='bilinear'), x)\n",
    "        concat1 = torch.cat((out_upconv1, disp2_up), 1)\n",
    "        out_iconv1 = self.iconv1(concat1)\n",
    "        disp1 = self.alpha * self.predict_disp1(out_iconv1) + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            return disp1, disp2, disp3, disp4, disp5, disp6\n",
    "        else:\n",
    "            return disp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "formed-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DispNetS6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(save_path, dispnet_state, filename='checkpoint.pth.tar'):\n",
    "    file_prefixes = ['dispnet']\n",
    "    states = [dispnet_state]\n",
    "    for (prefix, state) in zip(file_prefixes, states):\n",
    "        torch.save(state, save_path/'{}_{}'.format(prefix,filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
