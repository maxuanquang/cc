{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooperative-phoenix",
   "metadata": {},
   "source": [
    "# Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "agreed-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "parental-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_l1(x, q=0.5, eps=1e-2):\n",
    "    x = torch.pow((x.pow(2) + eps), q)\n",
    "    x = x.mean()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "suffering-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_l1_per_pix(x, q=0.5, eps=1e-2):\n",
    "    x = torch.pow((x.pow(2) + eps), q)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pretty-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Jonas Wulff\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous(), requires_grad=False)\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    return ssim_map\n",
    "    #if size_average:\n",
    "    #    return ssim_map.mean()\n",
    "    #else:\n",
    "    #    return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "\n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 13, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sound-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_img = torch.rand(4,3,256,832)\n",
    "\n",
    "ref_imgs = [torch.rand(4,3,256,832),torch.rand(4,3,256,832)]\n",
    "\n",
    "occ_masks = torch.rand(4,4,640,640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "genetic-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = 0\n",
    "h, w = 640, 640\n",
    "\n",
    "tgt_img_scaled = nn.functional.adaptive_avg_pool2d(tgt_img, (h, w))\n",
    "ref_imgs_scaled = [nn.functional.adaptive_avg_pool2d(ref_img, (h, w)) for ref_img in ref_imgs]\n",
    "\n",
    "weight = 0.5\n",
    "wssim = 0.85\n",
    "\n",
    "ssim_losses = []\n",
    "photometric_losses = []\n",
    "\n",
    "for i, ref_img in enumerate(ref_imgs_scaled):\n",
    "\n",
    "    valid_pixels = 1 - (ref_img == 0).prod(1, keepdim=True).type_as(ref_img)\n",
    "    diff = (tgt_img_scaled - ref_img) * valid_pixels\n",
    "    ssim_loss = 1 - ssim(tgt_img_scaled, ref_img) * valid_pixels\n",
    "    oob_normalization_const = valid_pixels.nelement()/valid_pixels.sum()\n",
    "\n",
    "    assert((oob_normalization_const == oob_normalization_const).item() == 1)\n",
    "\n",
    "    diff = diff *(1-occ_masks[:,i:i+1]).expand_as(diff)\n",
    "    ssim_loss = ssim_loss*(1-occ_masks[:,i:i+1]).expand_as(ssim_loss)\n",
    "\n",
    "    # reconstruction_loss +=  oob_normalization_const*((1- wssim)*robust_l1_per_pix(diff, q=qch) + weight*wssim*ssim_loss).min() + lambda_oob*robust_l1(1 - valid_pixels, q=qch)\n",
    "    ssim_losses.append(oob_normalization_const*weight*wssim*ssim_loss)\n",
    "    photometric_losses.append(oob_normalization_const*(1 - wssim)*robust_l1(diff))\n",
    "    # assert((reconstruction_loss == reconstruction_loss).item() == 1)\n",
    "    #weight /= 2.83\n",
    "\n",
    "ssim_losses = torch.stack(ssim_losses)\n",
    "photometric_losses = torch.stack(photometric_losses)\n",
    "reconstruction_loss = torch.min(ssim_losses,0)[0].mean() + torch.mean(photometric_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aggressive-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = 0\n",
    "h, w = 640, 640\n",
    "\n",
    "tgt_img_scaled = nn.functional.adaptive_avg_pool2d(tgt_img, (h, w))\n",
    "ref_imgs_scaled = [nn.functional.adaptive_avg_pool2d(ref_img, (h, w)) for ref_img in ref_imgs]\n",
    "\n",
    "weight = 0.5\n",
    "wssim = 0.85\n",
    "\n",
    "pe_losses = []\n",
    "\n",
    "for i, ref_img in enumerate(ref_imgs_scaled):\n",
    "\n",
    "    valid_pixels = 1 - (ref_img == 0).prod(1, keepdim=True).type_as(ref_img)\n",
    "    diff = (tgt_img_scaled - ref_img) * valid_pixels\n",
    "    ssim_loss = 1 - ssim(tgt_img_scaled, ref_img) * valid_pixels\n",
    "    oob_normalization_const = valid_pixels.nelement()/valid_pixels.sum()\n",
    "\n",
    "    assert((oob_normalization_const == oob_normalization_const).item() == 1)\n",
    "\n",
    "    diff = diff *(1-occ_masks[:,i:i+1]).expand_as(diff)\n",
    "    ssim_loss = ssim_loss*(1-occ_masks[:,i:i+1]).expand_as(ssim_loss)\n",
    "\n",
    "    # reconstruction_loss +=  oob_normalization_const*((1- wssim)*robust_l1_per_pix(diff, q=qch) + weight*wssim*ssim_loss).min() + lambda_oob*robust_l1(1 - valid_pixels, q=qch)\n",
    "    pe = oob_normalization_const*weight*wssim*ssim_loss + oob_normalization_const*(1 - wssim)*robust_l1_per_pix(diff, q=qch) + lambda_oob*robust_l1_per_pix(1 - valid_pixels, q=qch) \n",
    "    pe = pe.mean(1)\n",
    "    pe_losses.append(pe)\n",
    "    # assert((reconstruction_loss == reconstruction_loss).item() == 1)\n",
    "    #weight /= 2.83\n",
    "\n",
    "pe_losses = torch.stack(pe_losses)\n",
    "\n",
    "pe_losses = pe_losses.min(0)[0]\n",
    "\n",
    "reconstruction_loss = pe_losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "institutional-liberty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 640, 640])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssim_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "formal-compatibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0193)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photometric_losses.min(0)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "august-project",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 640, 640])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_l1_per_pix(diff).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "identified-conclusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 640, 640])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssim(tgt_img_scaled, ref_img).mean(1, True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "comfortable-richardson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1600)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_l1(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss +=  \n",
    "(1- wssim)*robust_l1(diff, q=qch) + wssim*ssim_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "announced-resource",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 832])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quang = [tgt_img.mean(1,True),tgt_img.mean(1,True),tgt_img.mean(1,True)]\n",
    "torch.cat(quang,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "historical-charm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1539)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fresh-headset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1528)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "executive-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometric_reconstruction_loss(tgt_img, ref_imgs, intrinsics, intrinsics_inv, depth, explainability_mask, pose, rotation_mode='euler', padding_mode='zeros', lambda_oob=0, qch=0.5, wssim=0.5):\n",
    "    if type(explainability_mask) not in [tuple, list]:\n",
    "        explainability_mask = [explainability_mask]\n",
    "    if type(depth) not in [list, tuple]:\n",
    "        depth = [depth]\n",
    "\n",
    "    loss = 0\n",
    "    for d, mask in zip(depth, explainability_mask):\n",
    "        occ_masks = depth_occlusion_masks(d, pose, intrinsics, intrinsics_inv)\n",
    "        loss += one_scale(d, mask, occ_masks)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-gregory",
   "metadata": {},
   "source": [
    "# Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "whole-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Niantic 2019. Patent Pending. All rights reserved.\n",
    "#\n",
    "# This software is licensed under the terms of the Monodepth2 licence\n",
    "# which allows for non-commercial use only, the full terms of which are made\n",
    "# available in the LICENSE file.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def disp_to_depth(disp, min_depth, max_depth):\n",
    "    \"\"\"Convert network's sigmoid output into depth prediction\n",
    "    The formula for this conversion is given in the 'additional considerations'\n",
    "    section of the paper.\n",
    "    \"\"\"\n",
    "    min_disp = 1 / max_depth\n",
    "    max_disp = 1 / min_depth\n",
    "    scaled_disp = min_disp + (max_disp - min_disp) * disp\n",
    "    depth = 1 / scaled_disp\n",
    "    return scaled_disp, depth\n",
    "\n",
    "\n",
    "def transformation_from_parameters(axisangle, translation, invert=False):\n",
    "    \"\"\"Convert the network's (axisangle, translation) output into a 4x4 matrix\n",
    "    \"\"\"\n",
    "    R = rot_from_axisangle(axisangle)\n",
    "    t = translation.clone()\n",
    "\n",
    "    if invert:\n",
    "        R = R.transpose(1, 2)\n",
    "        t *= -1\n",
    "\n",
    "    T = get_translation_matrix(t)\n",
    "\n",
    "    if invert:\n",
    "        M = torch.matmul(R, T)\n",
    "    else:\n",
    "        M = torch.matmul(T, R)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "def get_translation_matrix(translation_vector):\n",
    "    \"\"\"Convert a translation vector into a 4x4 transformation matrix\n",
    "    \"\"\"\n",
    "    T = torch.zeros(translation_vector.shape[0], 4, 4).to(device=translation_vector.device)\n",
    "\n",
    "    t = translation_vector.contiguous().view(-1, 3, 1)\n",
    "\n",
    "    T[:, 0, 0] = 1\n",
    "    T[:, 1, 1] = 1\n",
    "    T[:, 2, 2] = 1\n",
    "    T[:, 3, 3] = 1\n",
    "    T[:, :3, 3, None] = t\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def rot_from_axisangle(vec):\n",
    "    \"\"\"Convert an axisangle rotation into a 4x4 transformation matrix\n",
    "    (adapted from https://github.com/Wallacoloo/printipi)\n",
    "    Input 'vec' has to be Bx1x3\n",
    "    \"\"\"\n",
    "    angle = torch.norm(vec, 2, 2, True)\n",
    "    axis = vec / (angle + 1e-7)\n",
    "\n",
    "    ca = torch.cos(angle)\n",
    "    sa = torch.sin(angle)\n",
    "    C = 1 - ca\n",
    "\n",
    "    x = axis[..., 0].unsqueeze(1)\n",
    "    y = axis[..., 1].unsqueeze(1)\n",
    "    z = axis[..., 2].unsqueeze(1)\n",
    "\n",
    "    xs = x * sa\n",
    "    ys = y * sa\n",
    "    zs = z * sa\n",
    "    xC = x * C\n",
    "    yC = y * C\n",
    "    zC = z * C\n",
    "    xyC = x * yC\n",
    "    yzC = y * zC\n",
    "    zxC = z * xC\n",
    "\n",
    "    rot = torch.zeros((vec.shape[0], 4, 4)).to(device=vec.device)\n",
    "\n",
    "    rot[:, 0, 0] = torch.squeeze(x * xC + ca)\n",
    "    rot[:, 0, 1] = torch.squeeze(xyC - zs)\n",
    "    rot[:, 0, 2] = torch.squeeze(zxC + ys)\n",
    "    rot[:, 1, 0] = torch.squeeze(xyC + zs)\n",
    "    rot[:, 1, 1] = torch.squeeze(y * yC + ca)\n",
    "    rot[:, 1, 2] = torch.squeeze(yzC - xs)\n",
    "    rot[:, 2, 0] = torch.squeeze(zxC - ys)\n",
    "    rot[:, 2, 1] = torch.squeeze(yzC + xs)\n",
    "    rot[:, 2, 2] = torch.squeeze(z * zC + ca)\n",
    "    rot[:, 3, 3] = 1\n",
    "\n",
    "    return rot\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Layer to perform a convolution followed by ELU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = Conv3x3(in_channels, out_channels)\n",
    "        self.nonlin = nn.ELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.nonlin(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv3x3(nn.Module):\n",
    "    \"\"\"Layer to pad and convolve input\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_refl=True):\n",
    "        super(Conv3x3, self).__init__()\n",
    "\n",
    "        if use_refl:\n",
    "            self.pad = nn.ReflectionPad2d(1)\n",
    "        else:\n",
    "            self.pad = nn.ZeroPad2d(1)\n",
    "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BackprojectDepth(nn.Module):\n",
    "    \"\"\"Layer to transform a depth image into a point cloud\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, height, width):\n",
    "        super(BackprojectDepth, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\n",
    "        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n",
    "        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n",
    "                                      requires_grad=False)\n",
    "\n",
    "        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n",
    "                                 requires_grad=False)\n",
    "\n",
    "        self.pix_coords = torch.unsqueeze(torch.stack(\n",
    "            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n",
    "        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n",
    "        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\n",
    "                                       requires_grad=False)\n",
    "\n",
    "    def forward(self, depth, inv_K):\n",
    "        cam_points = torch.matmul(inv_K[:, :3, :3], self.pix_coords)\n",
    "        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\n",
    "        cam_points = torch.cat([cam_points, self.ones], 1)\n",
    "\n",
    "        return cam_points\n",
    "\n",
    "\n",
    "class Project3D(nn.Module):\n",
    "    \"\"\"Layer which projects 3D points into a camera with intrinsics K and at position T\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, height, width, eps=1e-7):\n",
    "        super(Project3D, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, points, K, T):\n",
    "        P = torch.matmul(K, T)[:, :3, :]\n",
    "\n",
    "        cam_points = torch.matmul(P, points)\n",
    "\n",
    "        pix_coords = cam_points[:, :2, :] / (cam_points[:, 2, :].unsqueeze(1) + self.eps)\n",
    "        pix_coords = pix_coords.view(self.batch_size, 2, self.height, self.width)\n",
    "        pix_coords = pix_coords.permute(0, 2, 3, 1)\n",
    "        pix_coords[..., 0] /= self.width - 1\n",
    "        pix_coords[..., 1] /= self.height - 1\n",
    "        pix_coords = (pix_coords - 0.5) * 2\n",
    "        return pix_coords\n",
    "\n",
    "\n",
    "def upsample(x):\n",
    "    \"\"\"Upsample input tensor by a factor of 2\n",
    "    \"\"\"\n",
    "    return F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "\n",
    "def get_smooth_loss(disp, img):\n",
    "    \"\"\"Computes the smoothness loss for a disparity image\n",
    "    The color image is used for edge-aware smoothness\n",
    "    \"\"\"\n",
    "    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "\n",
    "    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "\n",
    "    grad_disp_x *= torch.exp(-grad_img_x)\n",
    "    grad_disp_y *= torch.exp(-grad_img_y)\n",
    "\n",
    "    return grad_disp_x.mean() + grad_disp_y.mean()\n",
    "\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    \"\"\"Layer to compute the SSIM loss between a pair of images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
    "\n",
    "        self.refl = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.C1 = 0.01 ** 2\n",
    "        self.C2 = 0.03 ** 2\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.refl(x)\n",
    "        y = self.refl(y)\n",
    "\n",
    "        mu_x = self.mu_x_pool(x)\n",
    "        mu_y = self.mu_y_pool(y)\n",
    "\n",
    "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
    "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
    "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
    "\n",
    "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
    "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
    "\n",
    "        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\n",
    "\n",
    "\n",
    "def compute_depth_errors(gt, pred):\n",
    "    \"\"\"Computation of error metrics between predicted and ground truth depths\n",
    "    \"\"\"\n",
    "    thresh = torch.max((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25     ).float().mean()\n",
    "    a2 = (thresh < 1.25 ** 2).float().mean()\n",
    "    a3 = (thresh < 1.25 ** 3).float().mean()\n",
    "\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = torch.sqrt(rmse.mean())\n",
    "\n",
    "    rmse_log = (torch.log(gt) - torch.log(pred)) ** 2\n",
    "    rmse_log = torch.sqrt(rmse_log.mean())\n",
    "\n",
    "    abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n",
    "\n",
    "    sq_rel = torch.mean((gt - pred) ** 2 / gt)\n",
    "\n",
    "    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "exact-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Niantic 2019. Patent Pending. All rights reserved.\n",
    "#\n",
    "# This software is licensed under the terms of the Monodepth2 licence\n",
    "# which allows for non-commercial use only, the full terms of which are made\n",
    "# available in the LICENSE file.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "class ResNetMultiImageInput(models.ResNet):\n",
    "    \"\"\"Constructs a resnet model with varying number of input images.\n",
    "    Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    \"\"\"\n",
    "    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n",
    "        super(ResNetMultiImageInput, self).__init__(block, layers)\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "        num_layers (int): Number of resnet layers. Must be 18 or 50\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        num_input_images (int): Number of frames stacked as input\n",
    "    \"\"\"\n",
    "    assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n",
    "    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
    "    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
    "    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
    "\n",
    "    if pretrained:\n",
    "        loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n",
    "        loaded['conv1.weight'] = torch.cat(\n",
    "            [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
    "        model.load_state_dict(loaded)\n",
    "    return model\n",
    "\n",
    "\n",
    "class ResnetEncoder(nn.Module):\n",
    "    \"\"\"Pytorch module for a resnet encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, pretrained, num_input_images=1):\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "\n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "\n",
    "        resnets = {18: models.resnet18,\n",
    "                   34: models.resnet34,\n",
    "                   50: models.resnet50,\n",
    "                   101: models.resnet101,\n",
    "                   152: models.resnet152}\n",
    "\n",
    "        if num_layers not in resnets:\n",
    "            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
    "\n",
    "        if num_input_images > 1:\n",
    "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
    "        else:\n",
    "            self.encoder = resnets[num_layers](pretrained)\n",
    "\n",
    "        if num_layers > 34:\n",
    "            self.num_ch_enc[1:] *= 4\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = (input_image - 0.45) / 0.225\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        self.features.append(self.encoder.relu(x))\n",
    "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
    "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
    "\n",
    "        return self.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "lightweight-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def downsample_conv(in_planes, out_planes, kernel_size=3):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=2, padding=(kernel_size-1)//2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_planes, out_planes, kernel_size=kernel_size, padding=(kernel_size-1)//2),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def predict_disp(in_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, 1, kernel_size=3, padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv(in_planes, out_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def upconv(in_planes, out_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def crop_like(input, ref):\n",
    "    assert(input.size(2) >= ref.size(2) and input.size(3) >= ref.size(3))\n",
    "    return input[:, :, :ref.size(2), :ref.size(3)]\n",
    "\n",
    "\n",
    "class DispNetS6(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=10, beta=0.01):\n",
    "        super(DispNetS6, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        conv_planes = [32, 64, 128, 256, 512, 512, 512]\n",
    "        self.conv1 = downsample_conv(3,              conv_planes[0], kernel_size=7)\n",
    "        self.conv2 = downsample_conv(conv_planes[0], conv_planes[1], kernel_size=5)\n",
    "        self.conv3 = downsample_conv(conv_planes[1], conv_planes[2])\n",
    "        self.conv4 = downsample_conv(conv_planes[2], conv_planes[3])\n",
    "        self.conv5 = downsample_conv(conv_planes[3], conv_planes[4])\n",
    "        self.conv6 = downsample_conv(conv_planes[4], conv_planes[5])\n",
    "        self.conv7 = downsample_conv(conv_planes[5], conv_planes[6])\n",
    "\n",
    "        upconv_planes = [512, 512, 256, 128, 64, 32, 16]\n",
    "        self.upconv7 = upconv(conv_planes[6],   upconv_planes[0])\n",
    "        self.upconv6 = upconv(upconv_planes[0], upconv_planes[1])\n",
    "        self.upconv5 = upconv(upconv_planes[1], upconv_planes[2])\n",
    "        self.upconv4 = upconv(upconv_planes[2], upconv_planes[3])\n",
    "        self.upconv3 = upconv(upconv_planes[3], upconv_planes[4])\n",
    "        self.upconv2 = upconv(upconv_planes[4], upconv_planes[5])\n",
    "        self.upconv1 = upconv(upconv_planes[5], upconv_planes[6])\n",
    "\n",
    "        self.iconv7 = conv(upconv_planes[0] + conv_planes[5], upconv_planes[0])\n",
    "        self.iconv6 = conv(upconv_planes[1] + conv_planes[4], upconv_planes[1])\n",
    "        self.iconv5 = conv(upconv_planes[2] + conv_planes[3], upconv_planes[2])\n",
    "        self.iconv4 = conv(upconv_planes[3] + conv_planes[2], upconv_planes[3])\n",
    "        self.iconv3 = conv(1 + upconv_planes[4] + conv_planes[1], upconv_planes[4])\n",
    "        self.iconv2 = conv(1 + upconv_planes[5] + conv_planes[0], upconv_planes[5])\n",
    "        self.iconv1 = conv(1 + upconv_planes[6], upconv_planes[6])\n",
    "\n",
    "        self.predict_disp6 = predict_disp(upconv_planes[1])\n",
    "        self.predict_disp5 = predict_disp(upconv_planes[2])\n",
    "        self.predict_disp4 = predict_disp(upconv_planes[3])\n",
    "        self.predict_disp3 = predict_disp(upconv_planes[4])\n",
    "        self.predict_disp2 = predict_disp(upconv_planes[5])\n",
    "        self.predict_disp1 = predict_disp(upconv_planes[6])\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv1 = self.conv1(x)\n",
    "        out_conv2 = self.conv2(out_conv1)\n",
    "        out_conv3 = self.conv3(out_conv2)\n",
    "        out_conv4 = self.conv4(out_conv3)\n",
    "        out_conv5 = self.conv5(out_conv4)\n",
    "        out_conv6 = self.conv6(out_conv5)\n",
    "        out_conv7 = self.conv7(out_conv6)\n",
    "\n",
    "        out_upconv7 = crop_like(self.upconv7(out_conv7), out_conv6)\n",
    "        concat7 = torch.cat((out_upconv7, out_conv6), 1)\n",
    "        out_iconv7 = self.iconv7(concat7)\n",
    "\n",
    "        out_upconv6 = crop_like(self.upconv6(out_iconv7), out_conv5)\n",
    "        concat6 = torch.cat((out_upconv6, out_conv5), 1)\n",
    "        out_iconv6 = self.iconv6(concat6)\n",
    "        disp6 = self.alpha * self.predict_disp6(out_iconv6) + self.beta\n",
    "\n",
    "        out_upconv5 = crop_like(self.upconv5(out_iconv6), out_conv4)\n",
    "        concat5 = torch.cat((out_upconv5, out_conv4), 1)\n",
    "        out_iconv5 = self.iconv5(concat5)\n",
    "        disp5 = self.alpha * self.predict_disp5(out_iconv5) + self.beta\n",
    "\n",
    "        out_upconv4 = crop_like(self.upconv4(out_iconv5), out_conv3)\n",
    "        concat4 = torch.cat((out_upconv4, out_conv3), 1)\n",
    "        out_iconv4 = self.iconv4(concat4)\n",
    "        disp4 = self.alpha * self.predict_disp4(out_iconv4) + self.beta\n",
    "\n",
    "        out_upconv3 = crop_like(self.upconv3(out_iconv4), out_conv2)\n",
    "        disp4_up = crop_like(nn.functional.upsample(disp4, scale_factor=2, mode='bilinear'), out_conv2)\n",
    "        concat3 = torch.cat((out_upconv3, out_conv2, disp4_up), 1)\n",
    "        out_iconv3 = self.iconv3(concat3)\n",
    "        disp3 = self.alpha * self.predict_disp3(out_iconv3) + self.beta\n",
    "\n",
    "        out_upconv2 = crop_like(self.upconv2(out_iconv3), out_conv1)\n",
    "        disp3_up = crop_like(nn.functional.upsample(disp3, scale_factor=2, mode='bilinear'), out_conv1)\n",
    "        concat2 = torch.cat((out_upconv2, out_conv1, disp3_up), 1)\n",
    "        out_iconv2 = self.iconv2(concat2)\n",
    "        disp2 = self.alpha * self.predict_disp2(out_iconv2) + self.beta\n",
    "\n",
    "        out_upconv1 = crop_like(self.upconv1(out_iconv2), x)\n",
    "        disp2_up = crop_like(nn.functional.upsample(disp2, scale_factor=2, mode='bilinear'), x)\n",
    "        concat1 = torch.cat((out_upconv1, disp2_up), 1)\n",
    "        out_iconv1 = self.iconv1(concat1)\n",
    "        disp1 = self.alpha * self.predict_disp1(out_iconv1) + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            return disp1, disp2, disp3, disp4, disp5, disp6\n",
    "        else:\n",
    "            return disp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "outer-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetEncoder(18,'pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "applied-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(dispnet_state, filename='checkpoint.pth.tar'):\n",
    "    file_prefixes = ['dispnet']\n",
    "    states = [dispnet_state]\n",
    "    for (prefix, state) in zip(file_prefixes, states):\n",
    "        torch.save(state, '{}_{}'.format(prefix,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "numerical-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
